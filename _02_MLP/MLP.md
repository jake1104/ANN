# π”— MLP (Multi-Layer Perceptron) - λ‹¤μΈµ νΌμ…‰νΈλ΅ μ κ°λ…κ³Ό κµ¬ν„

## 1. κ°μ”

MLP(λ‹¤μΈµ νΌμ…‰νΈλ΅ )λ” λ‹¨μΈµ νΌμ…‰νΈλ΅ (SLP)μ ν•κ³„λ¥Ό κ·Ήλ³µν•κΈ° μ„ν•΄ κ³ μ•λ μ‹ κ²½λ§ κµ¬μ΅°λ΅, μ…λ ¥μΈµκ³Ό μ¶λ ¥μΈµ μ‚¬μ΄μ— ν•λ‚ μ΄μƒμ μ€λ‹‰μΈµ(hidden layer)μ„ κ°–λ” κµ¬μ΅°μ΄λ‹¤. λΉ„μ„ ν• ν™μ„±ν™” ν•¨μμ™€ λ‹¤μΈµ κµ¬μ΅°λ¥Ό ν†µν•΄ λΉ„μ„ ν•μ μΈ λ¬Έμ λ„ ν¨κ³Όμ μΌλ΅ ν•΄κ²°ν•  μ μλ‹¤. λ€ν‘μ μΌλ΅ SLPλ΅λ” ν’€ μ μ—†λ” XOR λ¬Έμ λ¥Ό MLPλ΅λ” ν•΄κ²°ν•  μ μλ‹¤.

---

## 2. MLPμ κµ¬μ΅° λ° μμ‹

### 2.1 λ μ΄μ–΄ κµ¬μ΅°

MLPλ” λ‹¤μκ³Ό κ°™μ€ ν•νƒμ λ μ΄μ–΄λ΅ κµ¬μ„±λλ‹¤:

* μ…λ ¥μΈµ: $x \in \mathbb{R}^{n_0}$
* μ€λ‹‰μΈµ(λ“¤): $h^{(l)} \in \mathbb{R}^{n_l}$, $l = 1, \dots, L-1$
* μ¶λ ¥μΈµ: $\hat{y} \in \mathbb{R}^{n_L}$

### 2.2 μμ „ν (Forward Propagation)

μ€λ‹‰μΈµ $l$μ—μ„μ μ—°μ‚°μ€ λ‹¤μκ³Ό κ°™μ΄ μ •μλλ‹¤:

$$
z^{(l)} = a^{(l-1)} W^{(l)} + b^{(l)} \\
a^{(l)} = f(z^{(l)})
$$

μ—¬κΈ°μ„,

* $W^{(l)} \in \mathbb{R}^{n_{l-1} \times n_l}$: κ°€μ¤‘μΉ ν–‰λ ¬
* $b^{(l)} \in \mathbb{R}^{1 \times n_l}$: νΈν–¥
* $f$: ν™μ„±ν™” ν•¨μ (sigmoid λ“±)
* $a^{(l)}$: lλ²μ§Έ μΈµμ μ¶λ ¥(λ‹¤μ μΈµμ μ…λ ¥)

μ¶λ ¥μΈµμ€ softmax ν•¨μλ¥Ό μ‚¬μ©ν•λ‹¤:

$$
\text{softmax}(z_j) = \frac{e^{z_j}}{\sum_{k=1}^{n} e^{z_k}}
$$

---

## 3. μ†μ‹¤ ν•¨μ

μ¶λ ¥μΈµμ΄ softmaxμ΄κ³  μ •λ‹µμ΄ one-hot μΈμ½”λ”©μΈ κ²½μ°, κµμ°¨ μ—”νΈλ΅ν”Ό μ†μ‹¤μ„ μ‚¬μ©ν•λ‹¤:

$$
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})
$$

* $N$: λ°μ΄ν„° μƒν” μ
* $C$: ν΄λμ¤ μ
* $y_{ij}$: μ •λ‹µ ν–‰λ ¬
* $\hat{y}_{ij}$: softmax ν™•λ¥  μ¶λ ¥

---

## 4. μ—­μ „ν (Backpropagation)

μ—­μ „νλ” κ° μΈµμ μ¤μ°¨λ¥Ό κ³„μ‚°ν•κ³ , μ΄λ¥Ό λ°”νƒ•μΌλ΅ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ—…λ°μ΄νΈν•λ” κ³Όμ •μ΄λ‹¤.

μ¶λ ¥μΈµλ¶€ν„° μ—­μμΌλ΅:

1. μ¶λ ¥μΈµμ gradient:

$$
\delta^{(L)} = \hat{y} - y
$$

2. μ€λ‹‰μΈµμ gradient:

$$
\delta^{(l)} = \left( \delta^{(l+1)} W^{(l+1)^\top} \right) \odot f'(z^{(l)})
$$

3. κ°€μ¤‘μΉ, νΈν–¥ μ—…λ°μ΄νΈ:

$$
W^{(l)} \leftarrow \text{Adam}(W^{(l)}, \nabla_{W^{(l)}} \mathcal{L}) \\
b^{(l)} \leftarrow \text{Adam}(b^{(l)}, \nabla_{b^{(l)}} \mathcal{L})
$$

μ—¬κΈ°μ„ $\odot$λ” μ”μ†λ³„ κ³±(Hadamard product)μ„ μλ―Έν•κ³ , κ°€μ¤‘μΉμ™€ νΈν–¥μ€ Adam μµν‹°λ§μ΄μ €λ¥Ό ν†µν•΄ μ—…λ°μ΄νΈλλ‹¤.

### 4.4 Adam μµν‹°λ§μ΄μ €

Adam(Adaptive Moment Estimation)μ€ κ° νλΌλ―Έν„°μ— λ€ν•΄ μ μ‘μ μΌλ΅ ν•™μµλ¥ μ„ μ΅°μ •ν•λ” μµμ ν™” μ•κ³ λ¦¬μ¦μ΄λ‹¤. μ΄μ „ κ·ΈλΌλ””μ–ΈνΈμ 1μ°¨ λ¨λ©νΈ(ν‰κ· )μ™€ 2μ°¨ λ¨λ©νΈ(λ¶„μ‚°)λ¥Ό ν™μ©ν•μ—¬ μ—…λ°μ΄νΈλ¥Ό μν–‰ν•λ‹¤.

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t = m_t / (1 - \beta_1^t) \\
\hat{v}_t = v_t / (1 - \beta_2^t) \\
\theta_t = \theta_{t-1} - \alpha \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)
$$

μ—¬κΈ°μ„ $g_t$λ” ν„μ¬ μ‹μ μ κ·ΈλΌλ””μ–ΈνΈ, $\alpha$λ” ν•™μµλ¥ , $\beta_1, \beta_2$λ” μ§€μ κ°€μ¤‘ ν‰κ·  κ³„μ, $\epsilon$μ€ λ¶„λ¨κ°€ 0μ΄ λλ” κ²ƒμ„ λ°©μ§€ν•λ” μ‘μ€ κ°’μ΄λ‹¤.

---

## 5. κµ¬ν„ μ½”λ“ μ„¤λ…

### 5.1 ν΄λμ¤ μ΄κΈ°ν™”

```python
self.ws = []
self.bs = []
for i in range(len(layer_sizes) - 1):
  w = random_matrix(input_dim, output_dim)
  b = zeros((1, output_dim))
```

* κ° λ μ΄μ–΄ κ°„μ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ΄κΈ°ν™”ν•λ‹¤.
* Xavier μ΄κΈ°ν™”λ¥Ό μ‚¬μ©ν•΄ ν•™μµ μ•μ •μ„±μ„ λ†’μΈλ‹¤.

---

### 5.2 μμ „ν κµ¬ν„

```python
def front_propagation(self, x):
  A = [x]
  Z = []
  ...
  z = dot(a_prev, W) + b
  a = sigmoid(z) or softmax(z)
```

* κ° μΈµμ— λ€ν•΄ zλ¥Ό κ³„μ‚°ν•κ³  ν™μ„±ν™” ν•¨μλ¥Ό ν†µκ³Όμ‹μΌ aλ¥Ό κ³„μ‚°ν•λ‹¤.
* λ§μ§€λ§‰ μΈµμ€ softmaxλ¥Ό μ‚¬μ©ν•μ—¬ ν™•λ¥  λ¶„ν¬λ¥Ό μ¶λ ¥ν•λ‹¤.

---

### 5.3 μ—­μ „ν κµ¬ν„

```python
dz = pred - y
for l in reversed(range(len(self.ws))):
  dw = dot(A[l].T, dZ)
  dZ = (dA @ Wαµ—) * sigmoid'(z)
```

* μ¶λ ¥μΈµμ μ¤μ°¨λ¶€ν„° μ‹μ‘ν•΄ κ° μΈµμ μ¤μ°¨λ¥Ό κ³„μ‚°ν•λ‹¤.
* κ³„μ‚°λ μ¤μ°¨μ™€ Adam μµν‹°λ§μ΄μ €λ¥Ό κΈ°λ°μΌλ΅ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ—…λ°μ΄νΈν•λ‹¤.
* `backward` λ©”μ„λ“λ” μƒμ„ λ μ΄μ–΄(μ: CNN)λ΅ μ „λ‹¬ν•κΈ° μ„ν• μ…λ ¥μΈµμ— λ€ν• κ·ΈλΌλ””μ–ΈνΈ(`d_input`)λ¥Ό λ°ν™ν•λ‹¤.

---

### 5.4 λ¨λΈ μ €μ¥ λ° λ¶λ¬μ¤κΈ°

```python
def save_model(self):
  np.savez("file.npz", w0=w0, b0=b0, w1=w1, b1=b1, ...)
```

* cupy λ°°μ—΄μ„ numpy λ°°μ—΄λ΅ λ³€ν™ν•μ—¬ μ €μ¥ν•λ‹¤.
* μ¶”ν›„ load\_model()μ—μ„ λ³µμ› κ°€λ¥
* `load_model` μ‹μ—λ” μ €μ¥λ `layer_sizes`μ™€ ν„μ¬ λ¨λΈμ κµ¬μ΅°λ¥Ό λΉ„κµν•μ—¬ λ¶μΌμΉ μ—¬λ¶€λ¥Ό ν™•μΈν•λ” λ΅μ§μ΄ ν¬ν•¨λμ–΄ λ¨λΈ λ΅λ”©μ μ•μ •μ„±μ„ λ†’μΈλ‹¤.

---

### 5.5 ν•™μµ ν•¨μ (train_standalone)

`train_standalone` ν•¨μλ” λ‹¤μκ³Ό κ°™μ€ νΉμ§•μ„ κ°–λ” λ…λ¦½μ μΈ ν•™μµ λ£¨ν”„λ¥Ό μ κ³µν•λ‹¤:

*   **λ―Έλ‹λ°°μΉ κ²½μ‚¬ ν•κ°•λ²•**: μ „μ²΄ λ°μ΄ν„°μ…‹μ„ μ‘μ€ λ°°μΉλ΅ λ‚λ„μ–΄ ν•™μµμ„ μ§„ν–‰ν•μ—¬ ν•™μµ μ†λ„λ¥Ό λ†’μ΄κ³  λ©”λ¨λ¦¬ ν¨μ¨μ„±μ„ κ°μ„ ν•λ‹¤.
*   **λ°μ΄ν„° μ…”ν”λ§**: κ° μ—ν¬ν¬λ§λ‹¤ λ°μ΄ν„°λ¥Ό λ¬΄μ‘μ„λ΅ μ„μ–΄ λ¨λΈμ΄ λ°μ΄ν„°μ μμ„μ— μμ΅΄ν•μ§€ μ•κ³  μΌλ°ν™” μ„±λ¥μ„ ν–¥μƒμ‹ν‚¤λ„λ΅ λ•λ”λ‹¤.
*   **μ΅°κΈ° μΆ…λ£ (Early Stopping)**: `target_loss` νλΌλ―Έν„°λ¥Ό ν†µν•΄ μ§€μ •λ μ†μ‹¤ κ°’μ— λ„λ‹¬ν•λ©΄ ν•™μµμ„ μ΅°κΈ°μ— μ¤‘λ‹¨ν•μ—¬ κ³Όμ ν•©μ„ λ°©μ§€ν•κ³  λ¶ν•„μ”ν• κ³„μ‚°μ„ μ¤„μΈλ‹¤.

---

### 5.6 νλΌλ―Έν„° μΈν„°νμ΄μ¤ (get_parameters, set_parameters)

*   `get_parameters()`: MLPμ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ λ‹¤λ¥Έ λ¨λΈ(μ: CNN)κ³Όμ νΈν™μ„±μ„ μ„ν•΄ νν” λ¦¬μ¤νΈ ν•νƒλ΅ λ°ν™ν•λ‹¤.
*   `set_parameters(params)`: μ™Έλ¶€λ΅λ¶€ν„° μ „λ‹¬λ°›μ€ νλΌλ―Έν„°(κ°€μ¤‘μΉ, νΈν–¥)λ¥Ό MLPμ— μ„¤μ •ν•λ‹¤. μ΄λ” MLPλ¥Ό λ” ν° μ‹ κ²½λ§ κµ¬μ΅°μ μΌλ¶€λ΅ μ‚¬μ©ν•  λ• μ μ©ν•λ‹¤.

## 6. μμ : XOR λ¬Έμ  ν•΄κ²°

```python
x = cp.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = cp.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding
```

XOR λ¬Έμ λ” μ„ ν•μ μΌλ΅ λ¶„λ¦¬ λ¶κ°€λ¥ν•λ―€λ΅ SLPλ΅λ” ν•΄κ²°ν•  μ μ—†μ§€λ§, MLPλ” μ΄λ¥Ό μ„±κ³µμ μΌλ΅ ν•™μµν•λ‹¤.

μμΈ΅ μ‹κ°ν™” κ²°κ³Όλ” λ‹¤μκ³Ό κ°™μ€ κ³΅μ„ μ  κ²°μ • κ²½κ³„λ¥Ό κ°€μ§„λ‹¤.

---

## 7. μ‹κ°ν™”

* μ…λ ¥ 2D κ³µκ°„μ„ κ·Έλ¦¬λ“λ΅ λ‚λ„κ³ , κ° μ§€μ μ—μ„ Class 1μ ν™•λ¥ μ„ μμΈ΅ν•μ—¬ contour plotμΌλ΅ μ‹κ°ν™”ν•λ‹¤.
* ν•™μµ λ°μ΄ν„°λ” μƒ‰μƒκ³Ό μμΈ΅κ°’μΌλ΅ ν‘μ‹λλ‹¤.

---

## 8. κ²°λ΅ 

MLPλ” μ‹ κ²½λ§μ κΈ°λ³Έμ μ΄λ©΄μ„λ„ κ°•λ ¥ν• κµ¬μ΅°μ΄λ‹¤. ν™μ„±ν™” ν•¨μ, μ†μ‹¤ ν•¨μ, μ—­μ „ν, weight initialization, λ¨λΈ μ €μ¥ λ“± μ‹¤μ©μ μΈ μ”μ†λ“¤μ„ ν¬ν•¨ν•¨μΌλ΅μ¨ ν•™μµ κ°€λ¥ν• λ‹¤μΈµ κµ¬μ΅°λ¥Ό μ™„μ„±ν•λ‹¤.

MLP_v04.py μ½”λ“λ” λ‹¤μκ³Ό κ°™μ€ νΉμ§•μ„ κ°–λ”λ‹¤:

* β… Cupyλ¥Ό ν™μ©ν• GPU κ°€μ†
* β… λ‹¤μΈµ κµ¬μ΅°μ μμ λ΅μ΄ μ„¤κ³„
* β… softmax + cross entropy μ΅°ν•©
* β… Adam μµν‹°λ§μ΄μ € μ μ©
* β… λ―Έλ‹λ°°μΉ ν•™μµ λ° λ°μ΄ν„° μ…”ν”λ§
* β… μ΅°κΈ° μΆ…λ£ (Early Stopping) κΈ°λ¥
* β… λ¨λΈ μ €μ¥ λ° λ΅λ“ μ‹ μ•„ν‚¤ν…μ² κ²€μ¦
* β… λ‹¤λ¥Έ λ¨λΈκ³Όμ νλΌλ―Έν„° μΈν„°νμ΄μ¤ (get_parameters, set_parameters)
* β… XOR λ¬Έμ  ν•΄κ²° κ°€λ¥
* β… μ‹κ°ν™” ν¬ν•¨
