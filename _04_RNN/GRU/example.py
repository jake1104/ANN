import argparse
import matplotlib.pyplot as plt
from .GRU_v01 import GRU, create_batches_for_embedding
import os
import numpy as np
import cupy as cp

usage_message = '''
================================================================================
GRU v1 (Adam Optimized) ì‚¬ìš©ë²•
================================================================================
- ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê·¸ëƒ¥ ì‹¤í–‰í•˜ë©´, ê¸°ì¡´ì— í•™ìŠµëœ 'gru_model.npz'ë¥¼ ë¶ˆëŸ¬ì™€ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.
- ì²˜ìŒë¶€í„° ìƒˆë¡œ í•™ìŠµí•˜ë ¤ë©´ `--force-train` ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- Adam ì˜µí‹°ë§ˆì´ì €ê°€ ì ìš©ë˜ì–´ í•™ìŠµì´ ë” ì•ˆì •ì ì´ê³  ë¹ ë¦…ë‹ˆë‹¤.

- ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ì:
  --plot             : í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
  --force-train      : ì €ì¥ëœ ëª¨ë¸ì„ ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„° ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.
  --epochs <ìˆ«ì>     : í•™ìŠµí•  ì—í¬í¬ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’: 2000)
  --batch-size <ìˆ«ì> : ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’: 4)
  --lr <ìˆ«ì>         : í•™ìŠµë¥ (learning rate)ì„ ì§€ì •í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’: 0.01)
  --target-loss <ìˆ«ì>: ì§€ì •ëœ ì†ì‹¤ ê°’ì— ë„ë‹¬í•˜ë©´ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.
  --load-path <ê²½ë¡œ>  : ì§€ì •ëœ ê²½ë¡œì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.
  --save-path <ê²½ë¡œ>  : í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ì„ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.

- ì˜ˆì‹œ:
  # ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ 500 ì—í¬í¬ ì¶”ê°€ í•™ìŠµ í›„ ê²°ê³¼ ì‹œê°í™”
  python -m ANN._04_RNN.GRU.example --epochs 500 --plot

  # íŠ¹ì • ê²½ë¡œì˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í•™ìŠµ í›„ ë‹¤ë¥¸ ê²½ë¡œì— ì €ì¥
  python -m ANN._04_RNN.GRU.example --load-path my_gru.npz --save-path new_gru.npz

  # ëª¨ë¸ì„ ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„° 1000 ì—í¬í¬ ì¬í•™ìŠµ
  python -m ANN._04_RNN.GRU.example --force-train --epochs 1000
================================================================================
'''
print(usage_message)

parser = argparse.ArgumentParser(description="Train and evaluate the GRU v1 with Adam.")
parser.add_argument('--plot', action='store_true', help='Show matplotlib plot for loss.')
parser.add_argument('--force-train', action='store_true', help='Force retraining even if a model file exists.')
parser.add_argument('--epochs', type=int, default=2000, help='Number of epochs to train. (Default: 2000)')
parser.add_argument('--batch-size', type=int, default=4, help='Batch size for training. (Default: 4)')
parser.add_argument('--lr', type=float, default=0.01, help='Learning rate for Adam. (Default: 0.01)')
parser.add_argument('--target-loss', type=float, default=None, help='Target loss to stop training early.')
parser.add_argument('--load-path', type=str, default=None, help='Path to load model and resume training.')
parser.add_argument('--save-path', type=str, default=None, help='Path to save the final model.')
args = parser.parse_args()

end_token = "\n"
sequences = [s + end_token for s in ["Banana", "Apple", "Melon", "Grape", "Peach", "Orange", "Cherry", "Strawberry"]]
corpus = "".join(sequences)
vocab = sorted(list(set(corpus)))
vocab_size = len(vocab)
item_to_idx = {item: i for i, item in enumerate(vocab)}
idx_to_item = {i: item for item, i in item_to_idx.items()}
end_idx = item_to_idx[end_token]

embedding_dim=16
n_h=64

batches = create_batches_for_embedding(sequences, item_to_idx, args.batch_size)
model = GRU(vocab_size=vocab_size, embedding_dim=embedding_dim, n_h=n_h, n_y=vocab_size)

model_name = "gru_model"
final_model_path = args.save_path if args.save_path else f"{model_name}.npz"
load_model_path = args.load_path if args.load_path else final_model_path
checkpoint_dir = "checkpoints"
start_epoch = 0

if not args.force_train and os.path.exists(load_model_path):
    print(f"\nPrevious model found. Loading weights and training state from {load_model_path}")
    try:
        start_epoch = model.load_model(load_model_path)
    except (ValueError, KeyError, EOFError) as e:
        print(f"Error loading model: {e}. Starting from scratch.")
        start_epoch = 0
else:
    print("\nNo previous model found or --force-train specified. Training from scratch.")

print("\nStarting training...")
loss_history, last_epoch = model.train(batches, epochs=args.epochs, lr=args.lr, print_interval=100, 
                                       save_every=500, checkpoint_dir=checkpoint_dir, 
                                       model_name=model_name, start_epoch=start_epoch, target_loss=args.target_loss)

print(f"\nSaving final model to {final_model_path}...")
model.save_model(final_model_path, epoch=last_epoch)

if args.plot and loss_history:
    total_epochs_trained = range(start_epoch, start_epoch + len(loss_history))
    plt.plot(total_epochs_trained, loss_history)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training Loss History (GRU v1 Adam)")
    plt.grid(True)
    plt.show()

print("\nğŸ”® Predicting with final model:")
for seed in ["Ba", "Ap", "Me", "Gr", "Pe", "Or", "Ch", "St"]:
    print(f"{seed} âœ {model.predict(seed, item_to_idx, idx_to_item, 10, end_idx).strip()}")
