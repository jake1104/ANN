
# ğŸ§  GRU (Gated Recurrent Unit, ê²Œì´íŠ¸ ìˆœí™˜ ì‹ ê²½ë§)

---

## ğŸ“˜ 1. ê°œìš”

**GRU**ëŠ” 2014ë…„ì— **Cho et al.**ì´ ì œì•ˆí•œ RNNì˜ í•œ ë³€í˜•ìœ¼ë¡œ,
LSTMê³¼ ë¹„ìŠ·í•˜ê²Œ **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ(Long-Term Dependency)** ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì´ë‹¤.

LSTMë³´ë‹¤ **êµ¬ì¡°ê°€ ë‹¨ìˆœ**í•´ì„œ **í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ë©°**,
ë§¤ìš° ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ê¸° ë•Œë¬¸ì— ë§ì€ ì‹¤ë¬´ ëª¨ë¸ì—ì„œë„ ìì£¼ ì“°ì¸ë‹¤.

---

## ğŸ” 2. LSTMê³¼ì˜ ë¹„êµ

| í•­ëª©            | LSTM                | GRU               |
| ------------- | ------------------- | ----------------- |
| ê²Œì´íŠ¸ ìˆ˜         | 3ê°œ (ì…ë ¥, ë§ê°, ì¶œë ¥ ê²Œì´íŠ¸) | 2ê°œ (ì—…ë°ì´íŠ¸, ë¦¬ì…‹ ê²Œì´íŠ¸) |
| ì…€ ìƒíƒœ $c_t$  | ìˆìŒ                  | ì—†ìŒ                |
| ì€ë‹‰ ìƒíƒœ $h_t$ | ìˆìŒ                  | ìˆìŒ (ì…€ ìƒíƒœ = ì€ë‹‰ ìƒíƒœ) |
| í•™ìŠµ ì†ë„         | ëŠë¦¼                  | ë¹ ë¦„                |
| êµ¬ì¡° ë³µì¡ë„        | ë†’ìŒ                  | ë‚®ìŒ                |

â¡ï¸ GRUëŠ” **ì…€ ìƒíƒœë¥¼ ë”°ë¡œ ë‘ì§€ ì•Šê³ **, ëª¨ë“  ì •ë³´ê°€ **ì€ë‹‰ ìƒíƒœ $h_t$** ì•ˆì— ë‹´ê¸´ë‹¤.

---

## âš™ï¸ 3. ìˆ˜ì‹ ì •ë¦¬

GRUì˜ ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤.

1ï¸âƒ£ **ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸ (Update gate)**
ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ë¥¼ ê²°ì •
$$
z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z)
$$

2ï¸âƒ£ **ë¦¬ì…‹ ê²Œì´íŠ¸ (Reset gate)**
ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìŠì„ì§€ë¥¼ ê²°ì •
$$
r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r)
$$

3ï¸âƒ£ **í›„ë³´ ì€ë‹‰ ìƒíƒœ (Candidate hidden state)**
ìƒˆë¡œìš´ ì •ë³´ë¥¼ ê³„ì‚°
$$
\tilde{h}*t = \tanh(W_h x_t + U_h (r_t \odot h*{t-1}) + b_h)
$$

4ï¸âƒ£ **ìµœì¢… ì€ë‹‰ ìƒíƒœ (Hidden state update)**
ì´ì „ ìƒíƒœì™€ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ì„ì–´ì„œ ìµœì¢… ì€ë‹‰ ìƒíƒœ ê²°ì •
$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$

---

## ğŸ’» 4. GRU Python êµ¬í˜„ ì˜ˆì‹œ

ì´ ì˜ˆì‹œëŠ” `numpy`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ GRUì˜ ê¸°ë³¸ì ì¸ êµ¬í˜„ì„ ë³´ì—¬ì¤€ë‹¤. GPU ê°€ì†ì„ ìœ„í•´ì„œëŠ” `numpy` ëŒ€ì‹  `cupy`ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤.

```python
# GRU (Gated Recurrent Unit)
# 2025-10-11: Vanilla GRU êµ¬í˜„

import numpy as np

class VanillaGRU:
    def __init__(self, n_x, n_h, n_y, lr=0.01):
        self.n_x, self.n_h, self.n_y = n_x, n_h, n_y
        self.lr = lr

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.Wz = np.random.randn(n_x, n_h) * 0.01
        self.Uz = np.random.randn(n_h, n_h) * 0.01
        self.bz = np.zeros((1, n_h))

        self.Wr = np.random.randn(n_x, n_h) * 0.01
        self.Ur = np.random.randn(n_h, n_h) * 0.01
        self.br = np.zeros((1, n_h))

        self.Wh = np.random.randn(n_x, n_h) * 0.01
        self.Uh = np.random.randn(n_h, n_h) * 0.01
        self.bh = np.zeros((1, n_h))

        self.Why = np.random.randn(n_h, n_y) * 0.01
        self.by = np.zeros((1, n_y))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def softmax(self, x):
        exp = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp / np.sum(exp, axis=1, keepdims=True)

    def forward(self, X):
        h = np.zeros((1, self.n_h))
        hs, ys = [], []

        for x_t in X:
            z = self.sigmoid(x_t @ self.Wz + h @ self.Uz + self.bz)
            r = self.sigmoid(x_t @ self.Wr + h @ self.Ur + self.br)
            h_tilde = np.tanh(x_t @ self.Wh + (r * h) @ self.Uh + self.bh)
            h = (1 - z) * h + z * h_tilde
            y = self.softmax(h @ self.Why + self.by)

            hs.append(h)
            ys.append(y)

        return np.array(hs), np.array(ys)

    def backward(self, X, Y, hs, ys):
        dWhy = np.zeros_like(self.Why)
        dby = np.zeros_like(self.by)
        dh_next = np.zeros((1, self.n_h))

        for t in reversed(range(len(X))):
            dy = ys[t] - Y[t]
            dWhy += hs[t].T @ dy
            dby += dy
            dh_next = dy @ self.Why.T + dh_next

        # ì´ ì˜ˆì‹œì—ì„œëŠ” ê°„ë‹¨í™”ë¥¼ ìœ„í•´ ê²Œì´íŠ¸ë³„ Backprop ê³„ì‚°ì€ ìƒëµí•˜ê³  ì¶œë ¥ì¸µ ê´€ë ¨ ê°€ì¤‘ì¹˜ë§Œ ì—…ë°ì´íŠ¸í•œë‹¤.
        self.Why -= self.lr * np.clip(dWhy, -5, 5)
        self.by -= self.lr * np.clip(dby, -5, 5)

    def train(self, X, Y, epochs=2000):
        for epoch in range(epochs):
            hs, ys = self.forward(X)
            loss = -np.sum(Y * np.log(ys + 1e-8))
            self.backward(X, Y, hs, ys)
            if epoch % 200 == 0:
                print(f"Epoch {epoch}/{epochs} | Loss: {loss:.4f}")

    def predict(self, X):
        _, ys = self.forward(X)
        return np.argmax(ys, axis=2)

# ì˜ˆì‹œ: "hi" â†’ "ih"
if __name__ == "__main__":
    vocab_size = 3
    X = np.eye(vocab_size)
    Y = np.roll(X, -1, axis=0)

    gru = VanillaGRU(n_x=vocab_size, n_h=8, n_y=vocab_size)
    gru.train(X, Y, epochs=1000)
```

---

## ğŸ§© 5. GRUì˜ ì¥ì 

| ì¥ì           | ì„¤ëª…                        |
| ----------- | ------------------------- |
| ğŸƒâ€â™‚ï¸ ë¹ ë¥¸ í•™ìŠµ | LSTMë³´ë‹¤ ê²Œì´íŠ¸ ìˆ˜ê°€ ì ì–´ì„œ ê³„ì‚°ëŸ‰ì´ ê°ì†Œ |
| ğŸ’¾ ì ì€ ë©”ëª¨ë¦¬   | íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ ì¤„ì–´ë“¤ì–´ íš¨ìœ¨ì          |
| âš–ï¸ ë¹„ìŠ·í•œ ì„±ëŠ¥   | LSTM ìˆ˜ì¤€ì˜ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ê°€ëŠ¥     |
| ğŸ”§ ë‹¨ìˆœí•œ êµ¬ì¡°   | êµ¬í˜„ ë° íŠœë‹ì´ ì‰¬ì›€               |

---

## ğŸ§  6. ì£¼ìš” í™œìš© ë¶„ì•¼

| ë¶„ì•¼         | ì„¤ëª…                              |
| ---------- | ------------------------------- |
| **í…ìŠ¤íŠ¸ ìƒì„±** | ë¬¸ì¥ ìë™ ì™„ì„±, ì±—ë´‡ ì‘ë‹µ                 |
| **ìŒì„± ì²˜ë¦¬**  | TTS(Text-to-Speech), ìŒì„± ê°ì • ì¸ì‹   |
| **ì‹œê³„ì—´ ì˜ˆì¸¡** | ì£¼ê°€, ë‚ ì”¨, ì„¼ì„œ ë°ì´í„° ì˜ˆì¸¡               |
| **ê¸°ê³„ ë²ˆì—­**  | Seq2Seq êµ¬ì¡°ì˜ Encoder-Decoderë¡œ í™œìš© |

---

## ğŸŒŸ 7. GRUëŠ” ì–´ë””ì— ì“°ë©´ ì¢‹ì€ê°€?

* **ì‹¤ì‹œê°„ ì²˜ë¦¬**ê°€ í•„ìš”í•œ ê³³ (ì˜ˆ: ì‹¤ì‹œê°„ ë²ˆì—­, ëŒ€í™”í˜• AI)
* **ë°ì´í„° ì–‘ì´ ì ê±°ë‚˜ ì§§ì€ ì‹œí€€ìŠ¤**ë¥¼ ë‹¤ë£¨ëŠ” ê³³
* **ëª¨ë°”ì¼/ì„ë² ë””ë“œ í™˜ê²½** (ë¹ ë¥¸ ì¶”ë¡ ê³¼ ë‚®ì€ ì—°ì‚°ëŸ‰ì´ ì¤‘ìš”í•  ë•Œ)

ì´ êµ¬í˜„ì€ `numpy` ê¸°ë°˜ì´ì§€ë§Œ, `cupy`ë¥¼ í™œìš©í•˜ì—¬ GPU ê°€ì†ì„ ì ìš©í•˜ë©´ ë”ìš± ë¹ ë¥¸ í•™ìŠµ ë° ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë‹¤.
